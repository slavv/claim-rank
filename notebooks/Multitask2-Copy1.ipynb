{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/slav/ai/claim-rank\n"
     ]
    }
   ],
   "source": [
    "%cd /home/slav/ai/claim-rank\n",
    "!export PYTHONPATH=.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42) # ! before importing keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data.debates import read_debates, Debate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.features.feature_sets import get_cb_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Conv1D, Dense\n",
    "from keras.models import Model, clone_model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from src.stats.rank_metrics import average_precision, precision_at_n, accuracy_rounded\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing.data import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokens_num', 'text_len', 'bag_tfidf', 'pos']\n"
     ]
    }
   ],
   "source": [
    "train_sentences = read_debates(Debate.FIRST) + read_debates(Debate.VP)\n",
    "val_sentences = read_debates(Debate.SECOND)\n",
    "test_sentences = read_debates(Debate.THIRD)\n",
    "\n",
    "pipeline = get_cb_pipeline(train=train_sentences)\n",
    "X_train = pipeline.fit_transform(train_sentences)\n",
    "X_val = pipeline.fit_transform(val_sentences)\n",
    "X_test = pipeline.transform(test_sentences)\n",
    "\n",
    "y_train_pf = np.array([int(s.labels[5]) for s in train_sentences])\n",
    "y_train_all = np.array([1 if s.label>0 else 0 for s in train_sentences])\n",
    "\n",
    "y_val_pf = np.array([int(s.labels[5]) for s in val_sentences])\n",
    "y_val_all = np.array([1 if s.label>0 else 0 for s in val_sentences])\n",
    "\n",
    "y_test_pf = np.array([int(s.labels[5]) for s in test_sentences])\n",
    "y_test_all = np.array([1 if s.label>0 else 0 for s in test_sentences])\n",
    "\n",
    "\n",
    "y_train_0 = np.array([int(s.labels[0]) for s in train_sentences])\n",
    "y_train_1 = np.array([int(s.labels[1]) for s in train_sentences])\n",
    "y_train_2 = np.array([int(s.labels[2]) for s in train_sentences])\n",
    "y_train_3 = np.array([int(s.labels[3]) for s in train_sentences])\n",
    "y_train_4 = np.array([int(s.labels[4]) for s in train_sentences])\n",
    "y_train_5 = np.array([int(s.labels[5]) for s in train_sentences])\n",
    "y_train_6 = np.array([int(s.labels[6]) for s in train_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2761, 1363)\n",
      "['0', '0', '1', '0', '0', '0', '0', '0', '0']\n",
      "1\n",
      "1363\n",
      "(2761,)\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(train_sentences[103].labels)\n",
    "print(train_sentences[103].label)\n",
    "print(X_train.shape[1])\n",
    "print(np.array(y_train_all).shape)\n",
    "print(y_val_pf[13])\n",
    "print(y_val_all[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_count = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_old(out_count):\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(200, activation='relu')(input_layer)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    \n",
    "    outputs = list(map(lambda _: Dense(1, activation='sigmoid')(x), range(out_count)))\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=outputs)\n",
    "\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(out_count):\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(200, activation='relu')(input_layer)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    \n",
    "    outputs = list(map(lambda _: Dense(1, activation='sigmoid')(x), range(out_count)))\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=outputs)\n",
    "    \n",
    "    sgd = optimizers.SGD(lr=0.01)\n",
    "                         \n",
    "    model.compile(optimizer=sgd, \n",
    "                  loss='mean_squared_logarithmic_error',\n",
    "                  metrics=['accuracy'])\n",
    "    return model;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_baseline_model(out_count):\n",
    "    input_layer = Input(shape=(features_count,))\n",
    "    x = Dense(features_count, kernel_initializer='normal', activation='relu')(input_layer)\n",
    "    \n",
    "    outputs = list(map(lambda _: Dense(1, kernel_initializer='normal', activation='sigmoid')(x), range(out_count)))\n",
    "\n",
    "    model = Model(inputs=[input_layer], outputs=outputs)\n",
    "                         \n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_single_model(train_target, label_func, iters = 20, epochs=5):\n",
    "    model = create_baseline_model(1);\n",
    "\n",
    "    best_model = None;\n",
    "    best_av_p = 0.;\n",
    "    best_epoch = -1;\n",
    "\n",
    "    for epoch in range(iters):\n",
    "        \n",
    "        model.fit(X_train, y=[train_target], epochs=epochs, verbose=0, batch_size=50)\n",
    "\n",
    "        predicted_val = model.predict(X_val)\n",
    "        \n",
    "        for pred, sentence in zip(predicted_val, val_sentences):\n",
    "            sentence.pred = pred[0]\n",
    "\n",
    "        av_p = average_precision(val_sentences,1, label_func)\n",
    "        \n",
    "        print(str(epoch) + ':' + str(av_p) + ' ', end='')\n",
    "        \n",
    "        if (av_p > best_av_p):\n",
    "            best_model = clone_model(model);\n",
    "            best_av_p = av_p;\n",
    "            best_epoch = epoch;\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(best_av_p)\n",
    "    print((best_epoch + 1) * epochs)\n",
    "\n",
    "    predicted_val = best_model.predict(X_test)\n",
    "    for pred, sentence in zip(predicted_val, test_sentences):\n",
    "        sentence.pred = pred[0]\n",
    "\n",
    "    print(average_precision(test_sentences,1,label_func))\n",
    "    \n",
    "    return [best_model, best_av_p, best_epoch]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_multiple_model(train_targets, iters = 20, epochs=5):\n",
    "    model = create_model_old(len(train_targets));\n",
    "\n",
    "    best_model = None;\n",
    "    best_av_p = 0.;\n",
    "    best_epoch = -1;\n",
    "\n",
    "    best_model_all = None;\n",
    "    best_av_p_all = 0.;\n",
    "    best_epoch_all = -1;\n",
    "\n",
    "    for epoch in range(iters):\n",
    "#         print(str(epoch) + ' ', end='')\n",
    "\n",
    "        model.fit(X_train, y=train_targets, epochs=epochs, verbose=0, batch_size=550)\n",
    "\n",
    "        predicted_val = model.predict(X_val)\n",
    "        \n",
    "        for pred, sentence in zip(predicted_val[1], val_sentences):\n",
    "            sentence.pred = pred[0]\n",
    "\n",
    "        av_p = average_precision(val_sentences,1, lambda x: x.labels[5])\n",
    "        if (av_p > best_av_p):\n",
    "            best_model = clone_model(model);\n",
    "            best_av_p = av_p;\n",
    "            best_epoch = epoch;\n",
    "\n",
    "        for pred, sentence in zip(predicted_val[0], val_sentences):\n",
    "            sentence.pred = pred[0]\n",
    "\n",
    "        av_p_all = average_precision(val_sentences,1)\n",
    "        if (av_p_all > best_av_p_all):\n",
    "            best_model_all = clone_model(model);\n",
    "            best_av_p_all = av_p_all;\n",
    "            best_epoch_all = epoch;\n",
    "    \n",
    "#     print()\n",
    "    \n",
    "#     print('All:')\n",
    "#     print(best_av_p_all)\n",
    "#     print((best_epoch_all+ 1) * epochs)\n",
    "\n",
    "#     predicted_val = best_model_all.predict(X_test)\n",
    "#     for pred, sentence in zip(predicted_val[0], test_sentences):\n",
    "#         sentence.pred = pred[0]\n",
    "\n",
    "#     print(average_precision(test_sentences,1))\n",
    "\n",
    "#     print('Pf:')\n",
    "#     print(best_av_p)\n",
    "#     print((best_epoch + 1) * epochs)\n",
    "\n",
    "#     predicted_val = best_model.predict(X_test)\n",
    "#     for pred, sentence in zip(predicted_val[1], test_sentences):\n",
    "#         sentence.pred = pred[0]\n",
    "\n",
    "#     print(average_precision(test_sentences,1,lambda x: x.labels[5]))\n",
    "    \n",
    "    return [[best_model, best_av_p, best_epoch], [best_model_all, best_av_p_all, best_epoch_all]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_multiple_times_single(train_targets, label_func, iters = 100):\n",
    "    best_model = None;\n",
    "    best_av_p = 0.;\n",
    "    best_epoch = -1;\n",
    "    \n",
    "    for i in range(iters):\n",
    "        model, av_p, epoch = run_single_model(train_targets, label_func)\n",
    "        print(str(i) + ':' + str(av_p) + ' ', end='')\n",
    "        if (av_p > best_av_p):\n",
    "            best_model = clone_model(model);\n",
    "            best_av_p = av_p;\n",
    "            best_epoch = epoch;\n",
    "    print()\n",
    "\n",
    "    print(best_av_p)\n",
    "    print(best_epoch )\n",
    "\n",
    "    predicted_val = best_model.predict(X_test)\n",
    "    for pred, sentence in zip(predicted_val, test_sentences):\n",
    "        sentence.pred = pred[0]\n",
    "\n",
    "    print(average_precision(test_sentences,1,label_func))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_multiple_times_multiple(train_targets, iters = 10):\n",
    "    best_model = None;\n",
    "    best_av_p = 0.;\n",
    "    best_epoch = -1;\n",
    "    \n",
    "    \n",
    "    best_model_all = None;\n",
    "    best_av_p_all = 0.;\n",
    "    best_epoch_all = -1;\n",
    "\n",
    "    for i in range(iters):\n",
    "        single_p, all_p = run_multiple_model(train_targets)\n",
    "        model, av_p, epoch = single_p\n",
    "        model_all, av_p_all, epoch_all = all_p\n",
    "        \n",
    "        print(str(i) + ':' + str(av_p) + ' ', end='')\n",
    "        print(str(i) + ':' + str(av_p_all) + ' ', end='')\n",
    "        \n",
    "        if (av_p > best_av_p):\n",
    "            best_model = clone_model(model);\n",
    "            best_av_p = av_p;\n",
    "            best_epoch = epoch;\n",
    "            \n",
    "        if (av_p_all > best_av_p_all):\n",
    "            best_model_all = clone_model(model_all);\n",
    "            best_av_p_all = av_p_all;\n",
    "            best_epoch_all = epoch_all;\n",
    "    print()\n",
    "    \n",
    "    print('All:')\n",
    "    print(best_av_p_all)\n",
    "    print(best_epoch_all)\n",
    "\n",
    "    predicted_val = best_model_all.predict(X_test)\n",
    "    for pred, sentence in zip(predicted_val[0], test_sentences):\n",
    "        sentence.pred = pred[0]\n",
    "\n",
    "    print(average_precision(test_sentences,1))\n",
    "\n",
    "    print('Pf:')\n",
    "    print(best_av_p)\n",
    "    print(best_epoch)\n",
    "\n",
    "    predicted_val = best_model.predict(X_test)\n",
    "    for pred, sentence in zip(predicted_val[1], test_sentences):\n",
    "        sentence.pred = pred[0]\n",
    "\n",
    "    print(average_precision(test_sentences,1,lambda x: x.labels[5]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(model, train_target, label_func):\n",
    "    # TRAINING\n",
    "    # Train the model using the training set\n",
    "    print(\"Start training model.\")\n",
    "    model.fit(X_train, train_target)\n",
    "    print(\"Finished training model.\")\n",
    "    \n",
    "    predicted_val = model.predict_proba(X_test)\n",
    "    predicted_val = MinMaxScaler().fit_transform([pred for pred in predicted_val]).tolist()\n",
    "    \n",
    "    for pred, sentence in zip(predicted_val, test_sentences):\n",
    "        sentence.pred = pred[1]\n",
    "    print(average_precision(test_sentences,1,label_func))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.22783351048080033 1:0.21717950936927102 2:0.21782584130876145 3:0.2180520253620826 4:0.21957998222546332 5:0.22106181629721455 6:0.21975023133292895 7:0.22036916446118443 8:0.21859539486813745 9:0.22169165117337858 10:0.21854871577047671 11:0.21998636543591313 12:0.219772840067682 13:0.21810878869934194 14:0.2149346491667143 15:0.21798468488583522 16:0.21529664849126318 17:0.21616102449780294 18:0.21378485905373315 19:0.21563903543820903 \n",
      "0.22783351048080033\n",
      "5\n",
      "0.17184869712609355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f6509bc0f60>, 0.22783351048080033, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_single_model(y_train_pf, lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_single_model(y_train_pf, lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.0816799441951372 1:0.08480889010507392 2:0.08525218022332721 3:0.07758946600993591 4:0.0844291711697922 5:0.08341415795280815 6:0.07818655212537882 7:0.07671115523926116 8:0.08190802339913694 9:0.08470770018305825 10:0.08515789723657782 11:0.07542268062140244 12:0.08051275984754143 13:0.08150382764420472 14:0.07863176478502215 15:0.08465545255244532 16:0.0775146081416086 17:0.08099810763761844 18:0.08301271143075784 19:0.08331193101883295 20:0.08476300293706106 21:0.08917111365296283 22:0.08004106735791279 23:0.08236426282364799 24:0.08188588917690619 25:0.08078060784645574 26:0.07979509696366892 27:0.07842703531229252 28:0.07798613972520517 29:0.08293304438017518 30:0.08726534347072322 31:0.08014815637331683 32:0.07932395212092351 33:0.07651561112911343 34:0.08210354762727619 35:0.07800099515720017 36:0.07465203267469875 37:0.0781846635109224 38:0.07731945859675415 39:0.08074471844769239 40:0.08176625362040837 41:0.08359451598039189 42:0.07681083885763104 43:0.08218580003973006 44:0.08153464294905656 45:0.08299665685234217 46:0.08012930469904336 47:0.07935983379558469 48:0.08097966591369826 49:0.07694726541132287 50:0.07763980385315171 51:0.08288106775494158 52:0.07811377227591693 53:0.08282355300451134 54:0.0795125831135393 55:0.07732094365444332 56:0.0827478188616001 57:0.08137958289189628 58:0.07562513450830523 59:0.08301712570717355 60:0.08074171686126658 61:0.0772420668399212 62:0.08034410827421605 63:0.08458768337995756 64:0.08021521747358872 65:0.07914592804704616 66:0.08455299161612943 67:0.08078230923684146 68:0.0793120930870251 69:0.08823435379621285 70:0.0799358774112099 71:0.07875933318110265 72:0.07847702043873823 73:0.0753815377890148 74:0.07992462409134335 75:0.08230034770233474 76:0.08054144594405643 77:0.08120946211604017 78:0.08307478900578143 79:0.07996751957730197 80:0.08591688897636969 81:0.08623382377670401 82:0.0834605003519011 83:0.07823022163314403 84:0.08031969462304407 85:0.07986889449968539 86:0.08012391760019134 87:0.08338353173113511 88:0.08172647220047229 89:0.07539539729086377 90:0.08350849314611684 91:0.08158990282207765 92:0.08248558759342182 93:0.07686019437004266 94:0.08332176089191824 95:0.08279518102000791 96:0.07604344301908701 97:0.08178286237910755 98:0.07574320382583007 99:0.0768462737167442 \n",
      "0.08917111365296283\n",
      "3\n",
      "0.04694245359348882\n"
     ]
    }
   ],
   "source": [
    "run_multiple_times_single(y_train_pf, lambda x: x.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.2959923566855628 1:0.26777804253569104 2:0.2800670805392862 3:0.2943152364170195 4:0.2771883196099317 5:0.29125992391567523 6:0.2807235991877473 7:0.2731229584755554 8:0.2818831248244777 9:0.2896156620154064 10:0.2864594621946322 11:0.273708727278938 12:0.2868464879117518 13:0.2967489018827364 14:0.26791465731906383 15:0.28970405429800866 16:0.2858319032442378 17:0.27021193736486426 18:0.29153916890443465 19:0.27769087741672205 20:0.2648802271542685 21:0.27823168193520503 22:0.3003503814861282 23:0.2914966010388375 24:0.28248189385092065 25:0.2703913242151129 26:0.28352241738380835 27:0.27671430227634325 28:0.2938076822148246 29:0.28837354479782684 30:0.28664716543155316 31:0.276575495519297 32:0.2975828587814472 33:0.28458889737938114 34:0.2841966626914128 35:0.28061875178547563 36:0.29155606634888476 37:0.28266493447070185 38:0.2691509814122788 39:0.2667443647188395 40:0.2927821313360341 41:0.284829970668317 42:0.28681973691962204 43:0.2690549699438455 44:0.28930565968003863 45:0.2960508704707128 46:0.2925428697343501 47:0.2713626852687234 48:0.26728752590459826 49:0.2764544342651956 50:0.2896667751169761 51:0.28002858841136896 52:0.2931695445404603 53:0.288024069263093 54:0.27654920465855104 55:0.2869400025290833 56:0.2862755572786784 57:0.27840039267785854 58:0.2701793985033524 59:0.30156770993298987 60:0.27736862180135946 61:0.290201509451101 62:0.2789442798379894 63:0.279508282372883 64:0.2873550909178497 65:0.27744581578388183 66:0.2891165487559084 67:0.27587649539240194 68:0.2783809190928755 69:0.28535416262021335 70:0.2894778861020982 71:0.2906147964924992 72:0.2897813128022182 73:0.28089729713079836 74:0.278824806088789 75:0.28958737543639673 76:0.28410303241233503 77:0.2845579148780106 78:0.28526702181776165 79:0.27173161494021186 80:0.2803950170095744 81:0.29919900413447187 82:0.27492113573159505 83:0.2719795764260648 84:0.2763567924439864 85:0.2881346772713518 86:0.27968081733938355 87:0.29965463655987573 88:0.26954419402256025 89:0.2883245475738573 90:0.27233872038003176 91:0.2911536913414036 92:0.27988017013495353 93:0.2766960146248817 94:0.29149486507687156 95:0.2773184803002549 96:0.2828500076886913 97:0.29568480865190866 98:0.2830876310120447 99:0.28709119825980733 \n",
      "0.30156770993298987\n",
      "1\n",
      "0.18704467919152173\n"
     ]
    }
   ],
   "source": [
    "run_multiple_times_single(y_train_all, lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.08476132129464271 0:0.3107075044291365 1:0.08558028898544857 1:0.290070129880259 2:0.08122200568448998 2:0.2958161539136481 3:0.07898196712028441 3:0.2748048626508349 4:0.08399431286900397 4:0.28752025976924384 5:0.08444418402970334 5:0.2947430872110886 6:0.09076107665826462 6:0.2979905027592441 7:0.08117146283232252 7:0.29025675683114033 8:0.08348613187177499 8:0.2875202195338798 9:0.08164491081485371 9:0.27987621383624584 10:0.08456899125978308 10:0.2938457355558174 11:0.0812880186338435 11:0.28701187845037257 12:0.08309202161545563 12:0.2734202648527543 13:0.0872137456180459 13:0.2744427295993051 14:0.08627683953377734 14:0.28202544422045156 15:0.08067430185346966 15:0.2903977735655574 16:0.08103911835787649 16:0.2861624352050825 17:0.08088987448659553 17:0.27026798995620005 18:0.08220503012739273 18:0.2833988789606126 19:0.07895470961932724 19:0.28457227542981356 20:0.0846209664109115 20:0.29432189713448953 21:0.09533205463829086 21:0.2888266052844089 22:0.08129304942164194 22:0.28533301947807627 23:0.07903774145912519 23:0.28857544553607617 24:0.08601980943822327 24:0.28446632588400117 25:0.0795015771303464 25:0.2962474813778037 26:0.09034120678662488 26:0.3017304944375999 27:0.08376761087756358 27:0.28604459593550996 28:0.0864821976794327 28:0.2895101839942505 29:0.0843011791559356 29:0.2772899747697608 30:0.0852113452465618 30:0.28016244256843803 31:0.09022072122166809 31:0.2860737641446743 32:0.07532854440255883 32:0.27713700949844416 33:0.08787376497917587 33:0.2807303217992099 34:0.0863146095294695 34:0.29925880158533946 35:0.08527602884903934 35:0.28090169985270536 36:0.07736697812981472 36:0.29124361578374425 37:0.08204997940319347 37:0.27929818528373085 38:0.08780282944393639 38:0.27269192455956043 39:0.08633673193082515 39:0.26874460508529346 40:0.0829683002849475 40:0.2962575919448159 41:0.08020831864201453 41:0.2861088080876952 42:0.08990489100296978 42:0.2764028172812758 43:0.08038368687153429 43:0.2893119979651481 44:0.08673608818914745 44:0.28386797564636007 45:0.07946661531814264 45:0.27741853828448304 46:0.08519247294254373 46:0.2859042319487415 47:0.08077082462129105 47:0.2757481881616943 48:0.08323204250032946 48:0.2946470670946286 49:0.083920185602022 49:0.2782299332215656 50:0.07975731853970873 50:0.3036737253405269 51:0.08737006744957518 51:0.2987177645151389 52:0.08407902325407911 52:0.27488224132556627 53:0.07851704156035205 53:0.29807379835060915 54:0.08149787616521556 54:0.2960313224001862 55:0.08512762727240139 55:0.2921008538458162 56:0.08377001437241137 56:0.28375397255527146 57:0.08832328714251386 57:0.2872860831053614 58:0.08255504388846625 58:0.27620602293891455 59:0.08847990961926486 59:0.2800258102496859 60:0.08451690033507779 60:0.28058805431048955 61:0.08614731287171076 61:0.28466907748222253 62:0.07799374574447619 62:0.2848376552527212 63:0.0819554630735654 63:0.2851609898495403 64:0.0806423450253559 64:0.28878996166944915 65:0.08540659745665362 65:0.293251188271513 66:0.08727594458073339 66:0.2814471674417027 67:0.08383789876148186 67:0.2862296844683417 68:0.08235610271930126 68:0.30127004748332303 69:0.08194357133000495 69:0.279927172512421 70:0.08092381628398927 70:0.27605298782113025 71:0.08859299498834158 71:0.28048715266915053 72:0.08182109137595828 72:0.28222896934188063 73:0.08350276285662063 73:0.2848315296069653 74:0.0844734848282237 74:0.2766625301440167 75:0.08533348979263028 75:0.2823687558681661 76:0.0828157325474228 76:0.2892241607782934 77:0.08651293070316174 77:0.28422076062906576 78:0.079279464342535 78:0.2726736205935586 79:0.08822162627668163 79:0.2822366396960141 80:0.08218940030357567 80:0.26799478088288986 81:0.07685272520437363 81:0.2813770472627 82:0.08330419865683612 82:0.2762874934451523 83:0.08714802448040902 83:0.3034480984037494 84:0.08370361335734691 84:0.28637842800033136 85:0.08344339704788331 85:0.2716188572713302 86:0.08537697084325085 86:0.2855291095535088 87:0.07890866363193383 87:0.28760396120988707 88:0.08263228815583215 88:0.29870547916174006 89:0.08144914582436984 89:0.28652387716708877 90:0.08061269523196594 90:0.27174298723548956 91:0.08631890378226675 91:0.2796896098359155 92:0.08547551746748307 92:0.26990336007148213 93:0.08096439584582305 93:0.28668621469006195 94:0.08302846911631097 94:0.2777280122210691 95:0.08697243562418974 95:0.27972942904054304 96:0.08451347294616862 96:0.2828751361074654 97:0.08292432053472638 97:0.29671446868780094 98:0.08906189413869785 98:0.2822442370105203 99:0.09048940257021529 99:0.2797555474832386 \n",
      "All:\n",
      "0.3107075044291365\n",
      "1\n",
      "0.19429367583811957\n",
      "Pf:\n",
      "0.09533205463829086\n",
      "2\n",
      "0.04864170877912805\n"
     ]
    }
   ],
   "source": [
    "run_multiple_times_multiple([y_train_all, y_train_pf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.08465472306775067 0:0.2828367741616345 1:0.08903517326863744 1:0.3061784807867154 2:0.09118600649398148 2:0.2983773160270636 3:0.08416155336100359 3:0.3053478497708418 4:0.08602868290833388 4:0.27771351418942675 5:0.08938549379197616 5:0.28756284969755436 6:0.08573422333073279 6:0.28785556951147534 7:0.08811954822924103 7:0.28793380329939455 8:0.08559455444590641 8:0.2805937986750401 9:0.08892448202942416 9:0.29913398410963954 \n",
      "All:\n",
      "0.3061784807867154\n",
      "2\n",
      "0.20186961996098074\n",
      "Pf:\n",
      "0.09118600649398148\n",
      "6\n",
      "0.05005328844904463\n"
     ]
    }
   ],
   "source": [
    "run_multiple_times_multiple([y_train_all, y_train_0, y_train_1, y_train_2, y_train_3, y_train_4, y_train_5, y_train_6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model.\n",
      "Finished training model.\n",
      "0.03147209533439084\n"
     ]
    }
   ],
   "source": [
    "run_model(SVC(class_weight='balanced', kernel='rbf', C=1, gamma=0.0001, probability=True, random_state=0),\n",
    "        y_train_pf, lambda x: x.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model.\n",
      "Finished training model.\n",
      "0.1275001023697541\n"
     ]
    }
   ],
   "source": [
    "run_model(SVC(class_weight='balanced', kernel='rbf', C=1, gamma=0.0001, probability=True, random_state=0),\n",
    "        y_train_all, lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model.\n",
      "Finished training model.\n",
      "0.06109935407157336\n"
     ]
    }
   ],
   "source": [
    " run_model(MLPClassifier(max_iter=100, solver='sgd', alpha=4, hidden_layer_sizes=(200, 50),\n",
    "                            random_state=1, activation='relu', learning_rate_init=0.03, batch_size=550),\n",
    "        y_train_pf, lambda x: x.labels[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training model.\n",
      "Finished training model.\n",
      "0.31253008848145997\n"
     ]
    }
   ],
   "source": [
    " run_model(MLPClassifier(max_iter=100, solver='sgd', alpha=4, hidden_layer_sizes=(200, 50),\n",
    "                            random_state=1, activation='relu', learning_rate_init=0.03, batch_size=550),\n",
    "        y_train_all, lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class MicroMock(object):\n",
    "#     def __init__(self, **kwargs):\n",
    "#         self.__dict__.update(kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = [[1,1,0.7],[0,1, 0.3]]\n",
    "# X_test = [[0,0.9, 0],[1,0.3, 1]]\n",
    "# y_train_pf = [1, 1]\n",
    "# test_sentences = [MicroMock(label=0), MicroMock(label=1)]\n",
    "# run_model(MLPClassifier(max_iter=10, solver='sgd', alpha=4, hidden_layer_sizes=(200, 50),\n",
    "#                             random_state=1, activation='relu', learning_rate_init=0.03, batch_size=550),\n",
    "#         y_train_pf, lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
